{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7dfde4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import datetime\n",
    "import kaggle\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e444d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declaracion de variables\n",
    "\n",
    "bucket_name = f'prueba-nequi-yheminson'\n",
    "data_set_1='sennikovandrey/data-netflix-prize'\n",
    "name_data_set_1='data-netflix-prize.zip'\n",
    "name_process='netflix'\n",
    "path_1='./download/'\n",
    "path_2='./destination/'\n",
    "path_3='./log/'\n",
    "path_4=f'./procesado_{name_process}/'\n",
    "current_date = datetime.datetime.now()\n",
    "access_key='AKIA6DEWUCC7WWLTEQ6Q'\n",
    "secret_key='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'\n",
    "\n",
    "# Manejo de logs\n",
    "logging.basicConfig(filename=f'{path_3}log_{name_process}.txt', format='%(asctime)s - %(levelname)s - %(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2e7bbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descarga DataSet\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "try:\n",
    "    os.makedirs(path_1)\n",
    "    logging.info(f'Se creo el directorio {path_1}')\n",
    "except:\n",
    "    logging.info(f'No se crea el directorio {path_1} porque ya existe')\n",
    "\n",
    "try:\n",
    "    # Descargar archivos utilizando kaggle.api\n",
    "    kaggle.api.dataset_download_files(data_set_1, path=path_1)\n",
    "    logging.info(f'DataSet {name_data_set_1} descargados exitosamente.')\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error al descargar el DataSet {name_data_set_1}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ec33ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descomprime el DataSet\n",
    "\n",
    "# Crear el directorio si no existe\n",
    "try:\n",
    "    os.makedirs(path_2)\n",
    "    logging.info(f'Se creo el directorio {path_2}')\n",
    "except:\n",
    "    logging.info(f'No se crea el directorio {path_2} porque ya existe')\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile(path_1 + name_data_set_1, 'r') as data_set_comp:\n",
    "        data_set_comp.extractall(path_2)\n",
    "    logging.info(f'Se descomprimio el DataSet {name_data_set_1}')\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"Error al descomprimir el DataSet {name_data_set_1}: {str(e)}\")\n",
    "    \n",
    "#Se borra el DataSet que se descargo y ya se descomprimio\n",
    "try:\n",
    "    os.remove(path_1 + name_data_set_1)\n",
    "    logging.info(f'Se elimina el DataSet {name_data_set_1}')\n",
    "    \n",
    "except Exception as e:\n",
    "    logging.error(f\"Error al borrar el DataSet {name_data_set_1}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cc4979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for root, dirs, files in os.walk(path_2):\n",
    "    for file in files:\n",
    "        if 'movie_genres.csv' in file or 'df_full.csv' in file:\n",
    "            path_2 = os.path.join(root, file)\n",
    "            \n",
    "            df_1 = pd.read_csv(path_2, on_bad_lines='skip', low_memory=False)\n",
    "            \n",
    "            # Crear el directorio si no existe\n",
    "            try:\n",
    "                os.makedirs(path_3)\n",
    "                logging.info(f'Se creo el directorio {path_3}')\n",
    "            except:\n",
    "                logging.info(f'No se crea el directorio {path_3} porque ya existe')\n",
    "            \n",
    "            #with open(f'{path_3}log.txt', 'a') as f:\n",
    "            # Verificar si hay valores perdidos en todo el DataFrame\n",
    "            logging.info(f'Valores perdidos en {file}:')\n",
    "            logging.info(df_1.isnull().sum())\n",
    "            #print(df_1.isnull().sum(), file=f)\n",
    "            #print('\\n\\n', file=f)\n",
    "\n",
    "            # Verificar duplicados en todo el DataFrame\n",
    "            if len (df_1[df_1.duplicated()]) > 0:\n",
    "                logging.info(f'Valores duplicados en todo el DataFrame en {file}:')\n",
    "                logging.info(df_1[df_1.duplicated()])\n",
    "                #print(df_1[df_1.duplicated()], file=f)\n",
    "                #print('\\n\\n', file=f)\n",
    "\n",
    "                # Elimina duplicados en el DataFrame\n",
    "                try:\n",
    "                    df_1 = df_1.drop_duplicates()\n",
    "                    logging.info(f'Se eliminan duplicados del DataSet {name_data_set_1}')\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error al eliminan duplicados del DataSet {name_data_set_1}: {str(e)}\")\n",
    "\n",
    "            # Verificar el tipo de datos de cada columna\n",
    "            logging.info(f'Tipos de datos de cada columna en {file}')\n",
    "            logging.info(df_1.dtypes)\n",
    "            #print(df_1.dtypes, file=f)\n",
    "            #print('\\n\\n', file=f)\n",
    "\n",
    "            # Eliminar registros null en el movie_title y movie_id\n",
    "            if 'movie_genres.csv' in file:\n",
    "                try:\n",
    "                    df_1.dropna(subset=['movieId'], inplace=True)\n",
    "                    df_1.dropna(subset=['title'], inplace=True)\n",
    "                    logging.info(f'Se eliminan registros null en el movieId y title en {file}')\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error al eliminan registros null en el movieId y title en {file}: {str(e)}\")\n",
    "\n",
    "            if 'df_full.csv' in file:\n",
    "                try:\n",
    "                    df_1.dropna(subset=['movie_id'], inplace=True)\n",
    "                    logging.info(f'Se eliminan registros null en el movie_id en {file}')\n",
    "                except Exception as e:\n",
    "                    logging.error(f\"Error al eliminar registros null en el movie_id en {file}: {str(e)}\")\n",
    "\n",
    "            # Sustituye los valores \\N por valores null\n",
    "\n",
    "            try:\n",
    "                df_1.replace(r'\\N', np.nan, inplace=True)\n",
    "                logging.info(f'Se sustituyen los valores sin data por valores null en {file}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error al sustituir los valores sin data por valores null en {file}: {str(e)}\")\n",
    "\n",
    "            # Guarda el DF\n",
    "\n",
    "            # Crear el directorio si no existe\n",
    "            try:\n",
    "                os.makedirs(path_4)\n",
    "                logging.info(f'Se creo el directorio {path_4}')\n",
    "            except:\n",
    "                logging.info(f'No se crea el directorio {path_4} porque ya existe')\n",
    "\n",
    "            try:\n",
    "                # df_1 = df_1.head(1000)\n",
    "                df_1.to_csv(f'{path_4}{os.path.splitext(file)[0]}.csv', index=False, header=True)\n",
    "                logging.info(f'Se guarda el DataFrame en  {path_4}{os.path.splitext(file)[0]}.csv')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error al guardar el DataFrame en  {path_4}{os.path.splitext(file)[0]}.csv: {str(e)}\")\n",
    "                \n",
    "            s3=boto3.client('s3', aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "\n",
    "            file_path = f'{path_4}{os.path.splitext(file)[0]}.csv'\n",
    "            key = f'{name_process}/{os.path.splitext(file)[0]}.csv'\n",
    "\n",
    "            s3.upload_file(file_path, bucket_name, key)\n",
    "\n",
    "            try:\n",
    "                s3.upload_file(file_path, bucket_name, key)\n",
    "                logging.info(f'DataSet cargado en el S3 {bucket_name}')\n",
    "            except Exception as e:\n",
    "                logging.error(f\"El archivo no existe en la ruta especificada {str(e)}\")\n",
    "\n",
    "        # Se borra el DataSet que ya se proceso\n",
    "        \n",
    "        try:\n",
    "            os.remove(os.path.join(root, file))\n",
    "            logging.info(f'Se borra el DataSet que ya se proceso {root}, {file}')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error al borrar el DataSet que ya se proceso {root}, {file}: {str(e)}\")\n",
    "\n",
    "# Se renombra el log\n",
    "os.rename(f'{path_3}log_{name_process}.txt', f'{path_3}log_{name_process}_{current_date.strftime(\"%Y%m%d%H%M%S\")}.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
